# Jetson Nano ALPR Optimization

High-performance Automatic License Plate Recognition (ALPR) for NVIDIA Jetson Nano using TensorRT optimization.

**Target Performance: 10-15 FPS** (vs 10 FPS baseline with ONNX Runtime)

## ðŸš€ Performance

| Component | ONNX Runtime | TensorRT .engine | Speedup |
|-----------|--------------|------------------|---------|
| Detector | ~60ms | ~35-40ms | **1.5-1.7x** |
| OCR | ~25ms | ~15-20ms | **1.3-1.5x** |
| **Total** | **~100ms (10 FPS)** | **~65-75ms (13-15 FPS)** | **1.3-1.5x** |

## ðŸŽ¯ Features

- **TensorRT Optimization**: Direct .engine conversion for maximum performance
- **FP16 Precision**: 2x throughput with minimal accuracy loss
- **Docker Deployment**: Production-ready containerized solution
- **Multi-Jetson Ready**: Easy deployment across multiple devices
- **Memory Efficient**: Optimized for 2GB Jetson Nano
- **Maintained Accuracy**: Detection >84%, OCR >97%

## ðŸ“‹ Requirements

- NVIDIA Jetson Nano
- JetPack 4.6+ (L4T R32.7.x)
- Docker with nvidia runtime
- 8GB+ SD card (16GB recommended)
- Sample images for testing

## ðŸ”§ Quick Start

### 1. Clone Repository

```bash
git clone https://github.com/yourusername/jetson-alpr-optimized.git
cd jetson-alpr-optimized
```

### 2. Build Docker Image

**Note**: This takes 10-15 minutes (includes TensorRT conversion)

```bash
sudo docker build -f Dockerfile.tensorrt -t jetson-alpr-tensorrt:latest .
```

### 3. Run Benchmark

```bash
sudo docker run --runtime nvidia --rm \
  -v /path/to/samples:/app/samples:ro \
  jetson-alpr-tensorrt:latest \
  python3 /app/benchmark_comparison.py
```

Expected output:
```
ONNX Runtime:  ~100ms (~10 FPS)
TensorRT:      ~70ms  (~14 FPS)
âœ“ TARGET ACHIEVED (10-15 FPS)
```

### 4. Process Images

```bash
# Single image
sudo docker run --runtime nvidia --rm \
  -v /path/to/samples:/app/samples:ro \
  jetson-alpr-tensorrt:latest \
  python3 /app/jetson_alpr_tensorrt.py /app/samples/image.jpg

# Directory of images
sudo docker run --runtime nvidia --rm \
  -v /path/to/samples:/app/samples:ro \
  -v $(pwd)/output:/app/output \
  jetson-alpr-tensorrt:latest \
  python3 /app/jetson_alpr_tensorrt.py /app/samples/
```

## ðŸ“¦ Multi-Jetson Deployment

### Save Image Once

```bash
# On first Jetson after building
sudo docker save jetson-alpr-tensorrt:latest | gzip > jetson-alpr-tensorrt.tar.gz
```

### Deploy to Other Jetsons

```bash
# Transfer
scp jetson-alpr-tensorrt.tar.gz user@jetson-ip:/tmp/

# Load on each Jetson
ssh user@jetson-ip 'sudo docker load < /tmp/jetson-alpr-tensorrt.tar.gz'
```

**Important**: TensorRT engines are hardware-specific. Build on Jetson Nano, not x86!

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input Image (e.g., 1920x1080)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Preprocessing & Letterbox Resize       â”‚
â”‚  â†’ 384x384 (detector input)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TensorRT Detector Engine (FP16)        â”‚
â”‚  YOLOv9-Tiny @ ~35-40ms                 â”‚
â”‚  â†’ Bounding boxes + confidence          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Extract Plate ROIs                     â”‚
â”‚  â†’ Resize to 140x70 (OCR input)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TensorRT OCR Engine (FP16)             â”‚
â”‚  MobileViT-v2 @ ~15-20ms                â”‚
â”‚  â†’ Character recognition                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Results: Plates + Text + Confidence    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ“ Project Structure

```
jetson-alpr-optimized/
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ DEPLOYMENT_README.md           # Detailed deployment guide
â”œâ”€â”€ OPTIMIZATION_SUMMARY.md        # Technical optimization details
â”œâ”€â”€ Dockerfile.tensorrt            # Optimized Dockerfile
â”œâ”€â”€ docker-compose.tensorrt.yml    # Docker Compose config
â”œâ”€â”€ convert_to_tensorrt.py         # ONNX â†’ TensorRT converter
â”œâ”€â”€ jetson_alpr_tensorrt.py        # TensorRT inference engine
â”œâ”€â”€ benchmark_comparison.py        # Performance comparison tool
â”œâ”€â”€ quick_test.sh                  # Quick test script
â””â”€â”€ .gitignore
```

## ðŸ”¬ Optimization Techniques

1. **TensorRT Engine Conversion**
   - ONNX â†’ .engine during Docker build
   - FP16 precision (2x faster)
   - Layer fusion & dead code elimination
   - Optimized memory allocation

2. **Inference Pipeline**
   - Direct TensorRT Runtime (no ONNX overhead)
   - PyCUDA zero-copy memory transfers
   - CUDA stream async execution
   - Minimal preprocessing

3. **Memory Management**
   - Conservative 256MB workspace
   - Optimized buffer allocation
   - GPU memory pooling
   - RAM: ~850MB total usage

## ðŸŽ›ï¸ Configuration

### Adjust Detection Confidence

Edit detector threshold for speed/accuracy trade-off:

```python
# In jetson_alpr_tensorrt.py
alpr = FastALPR(detector_engine, ocr_engine, conf_thresh=0.5)  # default: 0.4
```

Higher threshold = faster (fewer OCR calls), but may miss some plates.

### Reduce Input Resolution

Trade accuracy for speed by reducing detector input size:

```python
# In jetson_alpr_tensorrt.py LicensePlateDetectorTRT.__init__
self.input_size = (320, 320)  # default: (384, 384)
```

## ðŸ“Š Benchmarking

Run comprehensive benchmarks:

```bash
sudo docker run --runtime nvidia --rm \
  -v /path/to/samples:/app/samples:ro \
  jetson-alpr-tensorrt:latest \
  python3 /app/benchmark_comparison.py
```

Monitor GPU/CPU/Memory during inference:

```bash
sudo tegrastats
```

Expected stats:
```
RAM: 850/1972MB  CPU: [40%,35%,45%,40%]  GPU: 80%@921MHz  Temp: 42C
```

## ðŸ› Troubleshooting

### "Illegal instruction" error
- Cause: Running outside Docker or wrong ONNX Runtime
- Fix: Always use `sudo docker run --runtime nvidia`

### TensorRT engines not found
- Cause: Conversion failed during build
- Fix: Check build logs, increase swap if OOM

### Low FPS (<10)
- Check GPU usage: `sudo tegrastats`
- Ensure `--runtime nvidia` is used
- Verify CUDA is available: `nvidia-smi` (inside container)

### Out of memory during build
- Increase swap space:
  ```bash
  sudo fallocate -l 4G /swapfile
  sudo chmod 600 /swapfile
  sudo mkswap /swapfile
  sudo swapon /swapfile
  ```

## ðŸ“š Documentation

- [DEPLOYMENT_README.md](DEPLOYMENT_README.md) - Complete deployment guide
- [OPTIMIZATION_SUMMARY.md](OPTIMIZATION_SUMMARY.md) - Technical deep dive

## ðŸ™ Credits

Based on:
- [jetson-alpr](https://github.com/Ionut13gmail/jetson-alpr) - Original implementation
- NVIDIA TensorRT
- dustynv/l4t-ml Docker images

## ðŸ“„ License

MIT License - see LICENSE file for details

## ðŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Test on real Jetson Nano hardware
4. Submit pull request

## ðŸ“ž Support

- GitHub Issues: Report bugs and request features
- Discussions: Share experiences and ask questions

## ðŸ”® Roadmap

- [ ] INT8 quantization support (2-4x speedup)
- [ ] Multi-stream video processing
- [ ] REST API integration
- [ ] Cloud deployment guide
- [ ] Performance monitoring dashboard
